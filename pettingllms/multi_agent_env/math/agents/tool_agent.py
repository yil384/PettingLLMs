import copy
import logging
from typing import Any
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.utils.logger_config import get_multi_logger
from typing import List
from pettingllms.multi_agent_env.math.math_utils import extract_code, get_code_execution_output, test_if_eq
logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class ToolAgent(Agent):
    """
    Agent specialized for solving mathematical problems.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Math Solving Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)
  
        self.multi_logger = get_multi_logger()

    def update_from_env(self, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)
        agent_obs = getattr(env_data, "agent_observations", None)

        def as_text(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, list):
                return "\n".join([str(v) for v in value])
            return str(value)
        

        problem = getattr(state, "problem", None)
        code_solution = getattr(state, "code_generated_solution", None)
        code_extracted_answer = getattr(state, "code_extracted_answer", None)
        reasoning_solution = getattr(state, "reasoning_generated_solution", None)
        reasoning_extracted_answer = getattr(state, "reasoning_extracted_answer", None)
        
        need_generate = code_solution in (None, "") or code_extracted_answer in (None, "")

        if need_generate:
            formatted_prompt = (
                f"You are a helpful assistant that solves mathematical problems through step-by-step reasoning.\n\n"
                f"You need to think step by step and provide a complete solution using python code with clear mathematical reasoning.\n"
                f"Please write Python code to solve this problem.\n And you need to print the final answer in the code. Like if the final anwer is the variable x, you need to write ```print(x)```.\n"
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        else:
            formatted_prompt = (
                f"You are a helpful assistant that refines mathematical solutions through reasoning.\n\n"
                f"Problem:\n{problem}\n\n"
                f"Your previous reasoning solution:\n{as_text(code_solution)}\n\n And your extracted answer is {code_extracted_answer}.\n"
                f"But your extracted answer is mismatch with the answer generated by another LLM directly solve the problem using reasoning.\n"
                f"The reasoning agent's solution is {reasoning_solution}\n"
                f"The reasoning agent's answer is {reasoning_extracted_answer}\n"
                f"Please firstly judge the mismatch is caused by the reasoning agent's solution or your code solution. And then solve the problem again.\n"
            )
            
            formatted_prompt += (
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        
        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    
    def update_from_model(self, response: str):
        # Parse the response and update agent_data
        self.current_action = extract_code(response)
        return self.current_action

    async def step(self, env_data: Env, env_worker: Any = None):
        """
        Process the generated code solution and evaluate it against the ground truth.
        """
        generated_solution = self.current_action
        env_data.state.code_generated_solution = generated_solution
        # 先不设置提取答案，待执行代码后根据输出设置

        # 3) Evaluate correctness
        ground_truth_answer = env_data.state.ground_truth_answer
        is_correct = False
        code_execution_output = None
        try:
            # 执行代码（通过 ray worker）
            code_execution_output = await get_code_execution_output(
                generated_solution,
                timeout=40.0,
                ray_actor=env_worker,
            )
        except Exception as e:
            code_execution_output = f"error: {e}"
        # 记录执行输出
        try:
            env_data.state.code_execution_output = code_execution_output
        except Exception:
            pass
        
        if code_execution_output is not None and ground_truth_answer is not None:
            try:
                # 使用执行输出与标准答案进行比较
                extracted = str(code_execution_output).strip()
                env_data.state.code_extracted_answer = extracted
                is_correct = await test_if_eq(extracted, str(ground_truth_answer).strip())
                env_data.state.code_is_correct = bool(is_correct)
                
                if is_correct:
                    self.done = True
                    self.is_pass = True
                    
            except Exception as e:
                print(f"Warning: Failed to evaluate code solution: {e}")
                is_correct = False
                env_data.state.code_is_correct = False
        else:
            env_data.state.code_is_correct = False

        # 4) Update reward based on correctness
        if len(self.reward_history) > 0:
            self.agent_reward = float(is_correct) - self.reward_history[-1]
        else:
            self.agent_reward = float(is_correct)
        self.reward_history.append(float(is_correct))

    def calculate_reward(self, env_data: List[Env]) -> float:
        """
        Compute reward based on environment state.
        Uses correctness for reward calculation.
        """
        state = getattr(env_data[0], "state", None)
        correctness = 0.0

        if state is not None:
            is_correct = getattr(state, "is_correct", None)
            if isinstance(is_correct, bool):
                correctness = float(is_correct)

        # Record and return
        self.agent_reward = correctness
        self.reward_history.append(self.agent_reward)
        
        return self.agent_reward
    
    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
