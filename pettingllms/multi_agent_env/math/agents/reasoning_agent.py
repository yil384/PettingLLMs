import copy
import logging
from typing import Any
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.utils.logger_config import get_multi_logger
from typing import List
from pettingllms.rewards.math_utils.utils import extract_answer
from pettingllms.multi_agent_env.math.math_utils import evaluate_math_solution

logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class ReasoningAgent(Agent):
    """
    Agent specialized for solving mathematical problems.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Math Solving Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)
        
        # 初始化多日志系统
        self.multi_logger = get_multi_logger()

    def update_from_env(self, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)
        agent_obs = getattr(env_data, "agent_observations", None)

        def as_text(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, list):
                return "\n".join([str(v) for v in value])
            return str(value)
        

        problem = getattr(state, "problem", None)
        reasoning_solution = getattr(state, "reasoning_generated_solution", None)
        reasoning_extracted_answer = getattr(state, "reasoning_extracted_answer", None)
        code_solution = getattr(state, "code_generated_solution", None)
        code_extracted_answer = getattr(state, "code_extracted_answer", None)
        
        need_generate = reasoning_solution in (None, "") or reasoning_extracted_answer in (None, "")

        if need_generate:
            formatted_prompt = (
                f"You are a helpful assistant that solves mathematical problems through step-by-step reasoning.\n\n"
                f"You need to think step by step and provide a complete solution with clear mathematical reasoning.\n"
                f"Please provide your final answer in #### format.\n\n"
                f"Problem:\n{problem}\n\n"
                f"Put your final answer in the format of `#### <answer>`.\n"
                f"For the final answer, only output the answer after ####, no other text.\n"
                f"Example: `#### 123`\n\n"
            )
        else:
            formatted_prompt = (
                f"You are a helpful assistant that refines mathematical solutions through reasoning.\n\n"
                f"Problem:\n{problem}\n\n"
                f"Your previous reasoning solution:\n{as_text(reasoning_solution)}\n\n And your extracted answer is {reasoning_extracted_answer}.\n"
                f"But your extracted answer is mismatch with the answer generated by another LLM using python code to solve the problem.\n"
                f"The code agent's solution is {code_solution}\n"
                f"The code agent's answer is {code_extracted_answer}\n"
                f"Please firstly judge the mismatch is caused by the code agent's solution or your reasoning solution. And then solve the problem again.\n"
            )
            
            formatted_prompt += (
                f"Put your final answer in the format of `#### <answer>`.\n"
                f"For the final answer, only output the answer after ####, no other text.\n"
                f"Example: `#### 123`\n\n"
            )
        
        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    
    def update_from_model(self, response: str):
        # Parse the response and update agent_data
        self.current_action = response.strip()
        return self.current_action

    async def step(self, env_data: Env, env_worker: Any = None):
        """
        Process the generated reasoning solution and evaluate it against the ground truth.
        """
       
        generated_solution = self.current_action
        env_data.state.reasoning_generated_solution = generated_solution

        # 2) Extract answer from the reasoning solution
        extracted_answer = extract_answer(generated_solution)
        env_data.state.reasoning_extracted_answer = extracted_answer

        # 3) Evaluate correctness
        ground_truth_answer = env_data.state.ground_truth_answer
        is_correct = False
        
        if generated_solution is not None and ground_truth_answer is not None:
            try:
                # 使用本项目的 utils 进行一致的评估
                is_correct, extracted = await evaluate_math_solution(generated_solution, ground_truth_answer)
                env_data.state.reasoning_extracted_answer = extracted
                if not hasattr(env_data.state, 'reasoning_is_correct'):
                    env_data.state.reasoning_is_correct = is_correct
                else:
                    env_data.state.reasoning_is_correct = is_correct
                
                if is_correct:
                    self.done = True
                    
            except Exception as e:
                print(f"Warning: Failed to evaluate reasoning solution: {e}")
                is_correct = False
                if not hasattr(env_data.state, 'reasoning_is_correct'):
                    env_data.state.reasoning_is_correct = False
                else:
                    env_data.state.reasoning_is_correct = False
        else:
            if not hasattr(env_data.state, 'reasoning_is_correct'):
                env_data.state.reasoning_is_correct = False
            else:
                env_data.state.reasoning_is_correct = False

        # 4) Update reward based on correctness
        if len(self.reward_history) > 0:
            self.agent_reward = float(is_correct) - self.reward_history[-1]
        else:
            self.agent_reward = float(is_correct)
        self.reward_history.append(float(is_correct))

    def calculate_reward(self, env_data: List[Env]) -> float:
        """
        Compute reward based on environment state.
        Uses correctness for reward calculation.
        """
        state = getattr(env_data[0], "state", None)
        correctness = 0.0

        if state is not None:
            is_correct = getattr(state, "is_correct", None)
            if isinstance(is_correct, bool):
                correctness = float(is_correct)

        # Record and return
        self.agent_reward = correctness
        self.reward_history.append(self.agent_reward)
        
        return self.agent_reward
    
    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
