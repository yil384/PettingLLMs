"""
Frontend Design Task Dual-Agent Collaboration Test System

This system implements agent testing using HuggingFaceM4/WebSight dataset samples
and communicates with two sglang ports for different agents, following sweet_rl pattern.

Key Features:
1. Load test samples from HuggingFaceM4/WebSight dataset
2. Use sglang servers on different ports for agents
3. Implement sweet_rl style evaluation workflow
4. Generate evaluation metrics and results
"""

import os
import sys
import json
import tempfile
import requests
import time
import asyncio
from typing import Dict, List, Optional, Tuple, Any
from tqdm import tqdm


# Add current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from agents.visual_info_agent import ReviewAgent
from agents.code_genaration_agent import CodeGenerationAgent
from websight_env import WebEnv

# Import web rendering tools
try:
    from web_utils import (
        render_full_html, get_driver, replace_urls, load_dataset
    )
    WEB_UTILS_AVAILABLE = True
except ImportError:
    print("Warning: web rendering tools not available")
    WEB_UTILS_AVAILABLE = False


class SGLangAgentClient:
    """
    Client for communicating with sglang agents on different ports
    """
    
    def __init__(self, hostname: str = "localhost", code_port: int = 8000, visual_port: int = 8001):
        """
        Initialize sglang client
        
        Args:
            hostname: Server hostname
            code_port: Port for code generation agent
            visual_port: Port for visual analysis agent
        """
        self.hostname = hostname
        self.code_port = code_port
        self.visual_port = visual_port
        self.code_base_url = f"http://{hostname}:{code_port}/v1"
        self.visual_base_url = f"http://{hostname}:{visual_port}/v1"
        
    def _make_request(self, base_url: str, messages: List[Dict], max_tokens: int = 4096) -> str:
        """
        Make request to sglang server
        
        Args:
            base_url: Base URL for the server
            messages: Chat messages
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated response
        """
        try:
            payload = {
                "model": "default",  # sglang uses "default" model name
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 0.7,
                "stream": False
            }
            
            response = requests.post(
                f"{base_url}/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30  # Reduced timeout from 60 to 30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ Request failed with status {response.status_code}: {response.text}")
                return f"Error: HTTP {response.status_code} - Failed to get response from server"
                
        except requests.exceptions.Timeout:
            print(f"âŒ Request timeout: Server {base_url} did not respond within 30 seconds")
            return "Error: Server timeout - please check if sglang server is running"
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ Connection error: Cannot connect to {base_url}")
            print(f"   ğŸ’¡ Please check if sglang server is running on this address")
            return f"Error: Connection failed - {str(e)}"
        except Exception as e:
            print(f"âŒ Request error: {type(e).__name__}: {e}")
            return f"Error: {str(e)}"
    
    def get_code_response(self, messages: List[Dict]) -> str:
        """Get response from code generation agent"""
        return self._make_request(self.code_base_url, messages, max_tokens=8192)
    
    def get_visual_response(self, messages: List[Dict]) -> str:
        """Get response from visual analysis agent"""
        return self._make_request(self.visual_base_url, messages, max_tokens=4096)
    
    def test_connections(self) -> bool:
        """Test connections to both servers"""
        print("ğŸ”— Testing connections to sglang servers...")
        
        test_messages = [{"role": "user", "content": "Hello, please respond with 'OK'"}]
        
        code_ok = False
        visual_ok = False
        
        # Test code generation server
        print(f"ğŸ” Testing code generation server at {self.hostname}:{self.code_port}...")
        code_response = self.get_code_response(test_messages)
        if "Error:" not in code_response:
            print(f"âœ… Code generation server ({self.code_port}): Connected")
            print(f"   ğŸ“ Response: {code_response[:50]}...")
            code_ok = True
        else:
            print(f"âŒ Code generation server ({self.code_port}): {code_response}")
            
        # Test visual analysis server  
        print(f"ğŸ” Testing visual analysis server at {self.hostname}:{self.visual_port}...")
        visual_response = self.get_visual_response(test_messages)
        if "Error:" not in visual_response:
            print(f"âœ… Visual analysis server ({self.visual_port}): Connected")
            print(f"   ğŸ“ Response: {visual_response[:50]}...")
            visual_ok = True
        else:
            print(f"âŒ Visual analysis server ({self.visual_port}): {visual_response}")
            
        if not code_ok or not visual_ok:
            print("\nğŸ’¡ SGLangæœåŠ¡å™¨è¿æ¥å¤±è´¥çš„å¸¸è§è§£å†³æ–¹æ¡ˆ:")
            print("   1. å¯åŠ¨SGLangæœåŠ¡å™¨:")
            print(f"      python -m sglang.launch_server --model-path <model> --port {self.code_port}")
            print(f"      python -m sglang.launch_server --model-path <model> --port {self.visual_port}")
            print("   2. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€:")
            print(f"      curl http://{self.hostname}:{self.code_port}/health")
            print(f"      curl http://{self.hostname}:{self.visual_port}/health")
            print("   3. æ£€æŸ¥é˜²ç«å¢™å’Œç½‘ç»œè¿æ¥")
            print("   4. ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®")
            print("   5. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦å……è¶³")
            
        return code_ok and visual_ok


class FrontendDesignAgentGraph:
    """
    Frontend design test system using WebSight dataset and sglang servers
    """
    
    def __init__(
        self, 
        hostname: str = "localhost",
        code_port: int = 8000,
        visual_port: int = 8001,
        max_iterations: int = 3,
        temp_path: Optional[str] = None
    ):
        """
        Initialize test system
        
        Args:
            hostname: Server hostname
            code_port: Port for code generation agent
            visual_port: Port for visual analysis agent
            max_iterations: Maximum iterations per task
            temp_path: Temporary file path
        """
        self.client = SGLangAgentClient(hostname, code_port, visual_port)
        self.visual_agent = ReviewAgent()
        self.code_agent = CodeGenerationAgent(max_iterations=max_iterations)
        self.max_iterations = max_iterations
        self.temp_path = temp_path or "temp/"+str(time.time())
        
        # Initialize webdriver if web_utils is available
        self.driver = None
        if WEB_UTILS_AVAILABLE:
            print("ğŸ”„ Initializing WebDriver for HTML rendering...")
            self.driver = get_driver()
            if self.driver:
                print("âœ… WebDriver initialized successfully")
                # Test functionality
                try:
                    self.driver.get("data:text/html,<html><body><h1>Test</h1></body></html>")
                    print("âœ… WebDriver test successful")
                except Exception as test_e:
                    print(f"âš ï¸ WebDriver test failed: {test_e}")
                    print("ğŸ”§ Switching to mock rendering mode")
                    try:
                        self.driver.quit()
                    except:
                        pass
                    self.driver = None
            else:
                print("ğŸ”§ No WebDriver available, using mock rendering mode")
        else:
            print("âš ï¸ sweet_rl not available, using mock rendering mode")
        
    def render_html_to_image(self, html_code: str, task_id: str, iteration: int = 0) -> Tuple[Optional[str], Optional[str]]:
        """
        Render HTML code to image using sweet_rl tools
        
        Args:
            html_code: HTML code to render
            task_id: Task identifier
            iteration: Iteration number
            
        Returns:
            Tuple of (image_path, html_path)
        """
        if not self.driver or not WEB_UTILS_AVAILABLE:
            print("âš ï¸ WebDriver not available, using mock rendering")
            # Create mock paths and save HTML file
            import time
            current_time = time.time()
            html_path = os.path.join(self.temp_path, f"{task_id}_iter_{iteration}.html")
            image_path = os.path.join(self.temp_path, f"{task_id}_iter_{iteration}.png")
            
            try:
                # Save HTML file even in mock mode
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(html_code)
                print(f"ğŸ’¾ Mock HTML saved to: {html_path}")
                
                # Create mock image
                import base64
                black_pixel_png = base64.b64decode(
                    'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=='
                )
                with open(image_path, 'wb') as f:
                    f.write(black_pixel_png)
                print(f"ğŸ–¼ï¸ Mock image created: {image_path}")
                
                return image_path, html_path
            except Exception as mock_e:
                print(f"âŒ Mock rendering failed: {mock_e}")
                return None, None
            
        try:
            # Convert task_id to hash for env_id integer requirement
            env_id = hash(f"{task_id}_{iteration}") & 0x7FFFFFFF  # Ensure positive int
            result = render_full_html(
                self.driver, 
                html_code, 
                self.temp_path, 
                env_id=env_id
            )
            if result and result[0]:
                return result[0], result[1]  # Return both image_path and html_path
            return None, None
        except Exception as e:
            print(f"âŒ Rendering failed: {e}")
            return None, None
    
  
    def _get_agents_list(self) -> List[Tuple[str, Any, str]]:
        """
        Automatically detect agents and return them as a list with standardized names
        
        Returns:
            List of (agent_name, agent_instance, original_name) tuples
        """
        agents = []
        
        # Dynamically find all agent attributes
        agent_attrs = []
        for attr_name in dir(self):
            attr = getattr(self, attr_name)
            if attr_name.endswith('_agent') and hasattr(attr, 'reset') and hasattr(attr, 'update_from_env'):
                agent_attrs.append((attr_name, attr))
        
        # Sort for consistent ordering
        agent_attrs.sort(key=lambda x: x[0])
        
        # Create standardized names: agent1, agent2, etc.
        for i, (original_name, agent_instance) in enumerate(agent_attrs, 1):
            standardized_name = f"agent{i}"
            agents.append((standardized_name, agent_instance, original_name))
        
        return agents
    
    def _get_agent_client_method(self, original_agent_name: str):
        """
        Get the appropriate client method for an agent based on its original name
        
        Args:
            original_agent_name: Original name of the agent (e.g., 'code_agent', 'visual_agent')
            
        Returns:
            Client method to use for this agent
        """
        if 'code' in original_agent_name.lower():
            return self._safe_code_response
        elif 'visual' in original_agent_name.lower():
            return self._safe_visual_response
        else:
            # Default to code response for unknown agent types
            return self._safe_code_response
    
    def _safe_code_response(self, messages: List[Dict]) -> str:
        """Safe code response with fallback"""
        response = self.client.get_code_response(messages)
        if response.startswith("Error:"):
            # Mock response for testing
            return """
<html>
<head>
    <title>Mock Generated Page</title>
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; background: #f0f0f0; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Mock Generated Content</h1>
        <p>This is a mock response generated because the sglang server is not available.</p>
        <p>The system is running in demonstration mode.</p>
    </div>
</body>
</html>
            """.strip()
        return response
    
    def _safe_visual_response(self, messages: List[Dict]) -> str:
        """Safe visual response with fallback"""
        response = self.client.get_visual_response(messages)
        if response.startswith("Error:"):
            # Mock response for testing
            return "The current design looks good and matches the requirements well. The layout is clean and the styling is appropriate. This appears to be very close to the desired result."
        return response
    
    async def loop(
        self, 
        observation: Dict,
        step_idx: int = 0
    ) -> Dict[str, Any]:
        """
        Single-step multi-agent interaction loop
        
        This function handles one iteration of agent-environment interaction,
        focusing only on essential data updates: prompt, response, action, reward.
        
        Args:
            observation: Current observation from environment
            step_idx: Current step index
            
        Returns:
            Dictionary containing essential update data for all agents
        """
        # Automatically detect agents
        agents_info = self._get_agents_list()
        
        # Initialize step update data
        step_update_data = {}
        
        # Process each agent in sequence
        for agent_name, agent_instance, original_name in agents_info:
            # ============ AGENT PROCESSING ============
            # Update agent with environment observation
            agent_instance.update_from_env(observation, reward=0.0, done=False, info={})
            
            # Get prompt messages
            prompt_messages = agent_instance.chat_completions.copy()
            
            # Get appropriate client method for this agent
            client_method = self._get_agent_client_method(original_name)
            
            # Get model response
            response = client_method(prompt_messages)
            
            # Update agent with model response
            action = agent_instance.update_from_model(response)
            
            # Get agent-specific action data
            if hasattr(agent_instance, 'get_generated_html'):
                action_data = agent_instance.get_generated_html()
                action_type = "code"
            elif hasattr(agent_instance, 'get_suggestions_summary'):
                action_data = agent_instance.get_suggestions_summary()
                action_type = "visual"
            else:
                action_data = str(action.action) if hasattr(action, 'action') else str(action)
                action_type = "generic"
            
            # Store essential agent data
            step_update_data[agent_name] = {
                "original_name": original_name,
                "prompt": prompt_messages,
                "response": response,
                "action": action_data,
                "action_type": action_type,
                "reward": 0.0  # Will be updated after environment step
            }
        
        return step_update_data
    
    async def update_rewards(
        self,
        step_update_data: Dict[str, Any],
        env_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Update agent rewards after environment step
        
        Args:
            step_update_data: Data from loop() function
            env_results: Results from environment step containing rewards
            
        Returns:
            Updated step data with rewards
        """
        # Update rewards for each agent
        for agent_name in step_update_data:
            if agent_name in env_results.get('rewards', {}):
                step_update_data[agent_name]["reward"] = env_results['rewards'][agent_name]
            else:
                # Default reward if not specified
                step_update_data[agent_name]["reward"] = env_results.get('default_reward', 0.0)
        
        return step_update_data
    
    def evaluate_single_task(self, sample: Dict) -> Dict:
        """
        Evaluate a single task sample using the new simplified loop function
        
        Args:
            sample: Sample from WebSight dataset
            
        Returns:
            Evaluation results
        """

        task_id = sample["task_id"]
        print(f"\nğŸ¯ Evaluating task: {task_id}")
        for key, value in sample.items():
            print(f"ğŸ“ {key}")
        #print(f"ğŸ“ Task: {sample['ground_truth']}")
        print(f"ğŸ“ Description: {sample['problem_description'][:100]}...")
        
        # Initialize environment
        env = WebEnv(task=sample, max_turns=self.max_iterations * 2, temp_path=self.temp_path)
        
        # Reset all agents
        agents_info = self._get_agents_list()
        for _, agent_instance, _ in agents_info:
            agent_instance.reset()
        
        # Get initial observation
        obs, _ = env.reset()
        
        # Initialize results structure
        results = {
            "task_id": task_id,
            "task_description": sample["problem_description"],
            "ground_truth_html": sample["ground_truth"],
            "reference_image": obs["reference_image"],
            "success": False,
            "total_iterations": 0,
            "final_html": "",
            "termination_reason": "",
            "iterations": [],
            "agent_data": {},  # Store cumulative agent data
            "env_info_summary": [],  # Store summary of all environment info
            "html_files": {  # Store HTML file paths
                "reference_html_path": getattr(env, 'reference_html_path', None),
                "iteration_html_paths": []  # Store HTML paths for each iteration
            }
        }
        
        # Initialize agent data storage
        for agent_name, _, original_name in agents_info:
            results["agent_data"][agent_name] = {
                "original_name": original_name,
                "steps": [],
                "total_reward": 0.0
            }
        
        # Multi-step interaction loop (handled in evaluate_single_task)
        try:
            for iteration in range(self.max_iterations):
                print(f"  ğŸ”„ Iteration {iteration + 1}/{self.max_iterations}")
                
                # Run single-step loop to get agent actions
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                step_update_data = loop.run_until_complete(
                    self.loop(obs, step_idx=iteration)
                )
                loop.close()
                
                # Process each agent's action with environment
                iteration_result = {
                    "iteration": iteration + 1,
                    "agent_actions": {},
                    "current_image": None,
                    "current_html_path": None,
                    "reference_image": None,
                    "visual_suggestions": "",
                    "updated_html": "",
                    "visual_response": "",
                    "code_response": "",
                    "task_description": None,
                    "env_info": {}  # æ·»åŠ ç¯å¢ƒinfoä¿¡æ¯å­˜å‚¨
                }
                
                for agent_name, agent_data in step_update_data.items():
                    action_type = agent_data["action_type"]
                    action_data_value = agent_data["action"]
                    
                    # Take step in environment
                    obs, reward, done, info = env.step(action_type, action_data_value)
                    
                    # Update rewards
                    env_results = {"default_reward": reward}
                    step_update_data = asyncio.run(
                        self.update_rewards(step_update_data, env_results)
                    )
                    
                    # Store agent step data
                    agent_step = {
                        "prompt": agent_data["prompt"],
                        "response": agent_data["response"],
                        "action": agent_data["action"],
                        "action_type": action_type,
                        "reward": reward,
                        "info": info  # Add info field to agent_step
                    }
                    results["agent_data"][agent_name]["steps"].append(agent_step)
                    results["agent_data"][agent_name]["total_reward"] += reward
                    
                    # Store action data for backward compatibility (including info)
                    agent_data_with_info = agent_data.copy()
                    agent_data_with_info["info"] = info
                    iteration_result["agent_actions"][agent_name] = agent_data_with_info
                    
                    # Extract data from info (environment observation)
                    if action_type == "code":
                        iteration_result["updated_html"] = action_data_value
                        iteration_result["code_response"] = agent_data["response"]
                        iteration_result["current_image"] = info.get("current_image")
                        iteration_result["current_html_path"] = info.get("current_html_path")
                        iteration_result["reference_image"] = info.get("reference_image")
                        iteration_result["env_info"]["code"] = info  # Store code agent's env info
                    elif action_type == "visual":
                        iteration_result["visual_suggestions"] = info.get("visual_suggestions", action_data_value)
                        iteration_result["visual_response"] = agent_data["response"]
                        iteration_result["task_description"] = info.get("task_description")
                        iteration_result["env_info"]["visual"] = info  # Store visual agent's env info
                
                # Store HTML file path in results (from iteration_result)
                if iteration_result.get("current_html_path"):
                    results["html_files"]["iteration_html_paths"].append({
                        "iteration": iteration,
                        "html_path": iteration_result["current_html_path"],
                        "agent": "code",
                        "timestamp": time.time()
                    })
                
                # Check termination conditions (from iteration_result)
                if not iteration_result.get("current_image"):
                    print("  âŒ HTML rendering failed")
                    print(f"     ğŸ” Iteration result keys: {list(iteration_result.keys())}")
                    print(f"     ğŸ“ Updated HTML exists: {'Yes' if iteration_result.get('updated_html') else 'No'}")
                    if iteration_result.get("updated_html"):
                        updated_html = iteration_result["updated_html"]
                        print(f"     ğŸ“ HTML length: {len(updated_html)} characters")
                        print(f"     ğŸ“‹ HTML preview: {updated_html[:200]}...")
                    print(f"     ğŸš— WebDriver available: {'Yes' if hasattr(env, 'driver') and env.driver else 'No'}")
                    print(f"     ğŸ“ Temp path: {getattr(env, 'temp_path', 'Not set')}")
                    
                    # Try to get more info about the last code generation
                    code_response = iteration_result.get("code_response", "")
                    if code_response:
                        print(f"     ğŸ¤– Code agent response length: {len(code_response)} characters")
                        if "html" in code_response.lower():
                            print(f"     âœ… Response contains 'html' keyword")
                        else:
                            print(f"     âŒ Response doesn't contain 'html' keyword")
                    
                    results["termination_reason"] = "RENDERING_FAILED"
                    results["rendering_failure_details"] = {
                        "iteration_result_keys": list(iteration_result.keys()),
                        "html_available": bool(iteration_result.get("updated_html")),
                        "current_image_from_info": iteration_result.get("current_image"),
                        "current_html_path_from_info": iteration_result.get("current_html_path"),
                        "webdriver_available": hasattr(env, 'driver') and bool(env.driver),
                        "temp_path": getattr(env, 'temp_path', None),
                        "code_response_length": len(code_response) if code_response else 0
                    }
                    break
                
                # Check if design is satisfactory (using visual response)
                visual_response = iteration_result.get("visual_response", "")
                if self._is_design_satisfactory(visual_response):
                    print("  âœ¨ Design achieved satisfactory results!")
                    results["success"] = True
                    results["termination_reason"] = "TASK_COMPLETED"
                    break
                
                # Store iteration results
                results["iterations"].append(iteration_result)
                
                # Add environment info summary for this iteration
                results["env_info_summary"].append({
                    "iteration": iteration + 1,
                    "env_info": iteration_result["env_info"]
                })
                
                if done:
                    results["termination_reason"] = "ENV_DONE"
                    break
        
        except Exception as e:
            print(f"âŒ Evaluation failed: {e}")
            results["termination_reason"] = f"ERROR: {str(e)}"
        
        # Finalize results
        results["total_iterations"] = len(results["iterations"])
        
        # Get final HTML if available
        try:
            if hasattr(self, 'code_agent') and hasattr(self.code_agent, 'get_generated_html'):
                results["final_html"] = self.code_agent.get_generated_html()
        except:
            results["final_html"] = ""
        
        # Add final HTML file path information (from the last iteration_result)
        if results["iterations"] and results["iterations"][-1].get("current_html_path"):
            results["html_files"]["final_html_path"] = results["iterations"][-1]["current_html_path"]
        else:
            results["html_files"]["final_html_path"] = None
            
        # Summary of HTML files
        results["html_files"]["total_html_files"] = len(results["html_files"]["iteration_html_paths"])
        if results["html_files"]["reference_html_path"]:
            results["html_files"]["total_html_files"] += 1
            
        print(f"ğŸ’¾ Saved {results['html_files']['total_html_files']} HTML files for task {task_id}")
        
        # Cleanup
        env.cleanup()
        
        return results
    
    def _is_design_satisfactory(self, visual_response: str) -> bool:
        """Check if design is satisfactory"""
        satisfactory_keywords = [
            "perfect match", "very close", "already achieved", "perfectly matches",
            "no modifications needed", "design complete", "extremely close"
        ]
        
        response_lower = visual_response.lower()
        return any(keyword in response_lower for keyword in satisfactory_keywords)
    
    def run_evaluation(
        self, 
        num_samples: int = 10, 
        output_path: Optional[str] = None,
        dataset_version: str = "v0.2"
    ) -> Dict:
        """
        Run evaluation on WebSight dataset samples
        
        Args:
            num_samples: Number of samples to evaluate
            output_path: Path to save results
            dataset_version: WebSight dataset version
            
        Returns:
            Evaluation summary
        """
        print(f"ğŸš€ Starting Frontend Design Agent Evaluation")
        print(f"ğŸ“Š Samples: {num_samples}, Max iterations: {self.max_iterations}")
        
        # Test connections
        if not self.client.test_connections():
            error_msg = "Failed to connect to sglang servers"
            print(f"\nâŒ {error_msg}")
            print("ğŸ”§ System will continue in mock mode for testing purposes")
            # Don't return error, continue with mock responses
        
      
        samples = load_dataset(num_samples, dataset_version)
        
        if not samples:
            return {"error": "Failed to load dataset samples"}
        
        # Run evaluation
        all_results = []
        success_count = 0
        
        for i, sample in enumerate(tqdm(samples, desc="Evaluating tasks")):
            try:
                result = self.evaluate_single_task(sample)
                all_results.append(result)
                
                if result.get("success", False):
                    success_count += 1
                    
                print(f"  âœ… Task {i+1}/{len(samples)} completed. Success: {result.get('success', False)}")
                
            except Exception as e:
                print(f"  âŒ Task {i+1} failed: {e}")
                all_results.append({
                    "task_id": sample.get("task_id", f"task_{i}"),
                    "error": str(e)
                })
        
        # Calculate metrics
        total_tasks = len(samples)
        success_rate = success_count / total_tasks if total_tasks > 0 else 0
        avg_iterations = sum(r.get("total_iterations", 0) for r in all_results) / total_tasks
        
        summary = {
            "total_tasks": total_tasks,
            "successful_tasks": success_count,
            "success_rate": success_rate,
            "average_iterations": avg_iterations,
            "detailed_results": all_results
        }
        
        # Save results
        if output_path:
            with open(output_path, 'w') as f:
                json.dump(summary, f, indent=2)
            print(f"ğŸ’¾ Results saved to: {output_path}")
        
        print(f"\nğŸ¯ Evaluation Summary:")
        print(f"  ğŸ“Š Total tasks: {total_tasks}")
        print(f"  âœ… Successful: {success_count}")
        print(f"  ğŸ“ˆ Success rate: {success_rate:.2%}")
        print(f"  ğŸ”„ Average iterations: {avg_iterations:.1f}")
        
        return summary
    
    def cleanup(self):
        """Cleanup resources"""
        if self.driver:
            try:
                self.driver.quit()
                print("âœ… WebDriver closed successfully")
            except Exception as e:
                print(f"âš ï¸ Error closing WebDriver: {e}")


def main():
    """
    Main function for running the test system
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="Frontend Design Agent Test System")
    parser.add_argument("--hostname", default="localhost", help="SGLang server hostname")
    parser.add_argument("--code_port", type=int, default=8000, help="Code generation agent port")
    parser.add_argument("--visual_port", type=int, default=8001, help="Visual analysis agent port")
    parser.add_argument("--num_samples", type=int, default=10, help="Number of samples to test")
    parser.add_argument("--max_iterations", type=int, default=3, help="Maximum iterations per task")
    parser.add_argument("--output_path", help="Path to save evaluation results")
    parser.add_argument("--temp_path", help="Temporary directory for files")
    parser.add_argument("--dataset_version", default="v0.2", help="WebSight dataset version")
    
    args = parser.parse_args()
    
    # Create test system
    test_system = FrontendDesignAgentGraph(
        hostname=args.hostname,
        code_port=args.code_port,
        visual_port=args.visual_port,
        max_iterations=args.max_iterations,
        temp_path=args.temp_path
    )
    
    try:
        # Run evaluation
        results = test_system.run_evaluation(
            num_samples=args.num_samples,
            output_path=args.output_path,
            dataset_version=args.dataset_version
        )
        
        if "error" in results:
            print(f"âŒ Evaluation failed: {results['error']}")
            return 1
        
        print("ğŸ‰ Evaluation completed successfully!")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Evaluation interrupted by user")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return 1
    finally:
        test_system.cleanup()


if __name__ == "__main__":
    exit(main()) 