import logging
import copy
import json
from typing import Any, Dict, Optional, List
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)

@dataclass
class PyCheckerEnvState:
    """State for PyChecker RL Environment"""
    problem_input: str = None  # Problem description + module header
    spec: str = None  # Module header (spec)
    golden_output: str = None  # Golden Verilog DUT
    circuit_type: str = "CMB"  # CMB or SEQ
    
    generated_python_code: str = None
    generated_python_code_list: List[str] = field(default_factory=list)
    
    # Legacy fields (for backward compatibility, prefer agent-specific fields)
    code_extracted: bool = False
    code_runs: bool = False
    output_matches: bool = False
    error_message: str = ""
    reward: float = 0.0
    
    # Gen TB Agent specific state
    tb_extracted: bool = False
    tb_runs: bool = False
    tb_stimulus_created: bool = False
    tb_error_message: str = ""
    
    # PyChecker Agent specific state
    pychecker_extracted: bool = False
    pychecker_runs: bool = False
    pychecker_matches: bool = False
    pychecker_error_message: str = ""
    
    # Multi-agent: paths shared between agents
    task_folder: str = None  # Task folder path (created by gen_tb_agent)
    stimulus_json_path: str = None  # Stimulus path (generated by gen_tb_agent)
    
    def to_dict(self):
        """Convert state to dictionary for logging."""
        return {
            'problem_input': self.problem_input[:200] + '...' if self.problem_input and len(self.problem_input) > 200 else self.problem_input,
            'circuit_type': self.circuit_type,
            'code_extracted': self.code_extracted,
            'code_runs': self.code_runs,
            'output_matches': self.output_matches,
            'reward': self.reward,
            'error_message': self.error_message,
            'code_count': len(self.generated_python_code_list),
        }
    
    def to_dict_compact(self, agent_name: str = None):
        """
        Convert state to compact dictionary for reduced logging.
        
        Args:
            agent_name: Optional agent name to return agent-specific state
                       ('gen_tb_agent' or 'pychecker_agent')
        """
        if agent_name == 'gen_tb_agent':
            return {
                'extracted': self.tb_extracted,
                'runs': self.tb_runs,
                'stimulus_created': self.tb_stimulus_created,
                'error_message': self.tb_error_message[:100] if self.tb_error_message else '',
                'count': len(self.generated_python_code_list)
            }
        elif agent_name == 'pychecker_agent':
            return {
                'extracted': self.pychecker_extracted,
                'runs': self.pychecker_runs,
                'matches': self.pychecker_matches,
                'error_message': self.pychecker_error_message[:100] if self.pychecker_error_message else '',
                'reward': self.reward,
                'count': len(self.generated_python_code_list)
            }
        else:
            # Legacy format for backward compatibility
            return {
                'tb_extracted': self.tb_extracted,
                'tb_runs': self.tb_runs,
                'tb_stimulus_created': self.tb_stimulus_created,
                'pychecker_extracted': self.pychecker_extracted,
                'pychecker_runs': self.pychecker_runs,
                'pychecker_matches': self.pychecker_matches,
                'reward': self.reward,
                'count': len(self.generated_python_code_list)
            }


class PyCheckerEnv:
    """
    Environment for PyChecker Python code generation with single-agent interaction.
    
    This environment handles Python GoldenDUT generation where an agent generates
    Python code and receives feedback based on simulation correctness.
    """

    def __init__(
        self, 
        env_idx: int,
        rollout_idx: int,
        max_turns: int,
        config: dict | None = None,
    ):
        """
        Initialize the PyChecker environment.
        """
        self.env_idx = env_idx
        self.rollout_idx = rollout_idx
        self.max_turns = max_turns
        self.config = config
        self.state = PyCheckerEnvState()
        self.current_turn = 0
        self.done = False

    def reset(self):
        """Reset the environment state"""
        self.state.generated_python_code = None
        self.state.generated_python_code_list = []
        # Legacy fields
        self.state.code_extracted = False
        self.state.code_runs = False
        self.state.output_matches = False
        self.state.error_message = ""
        self.state.reward = 0.0
        # Gen TB Agent fields
        self.state.tb_extracted = False
        self.state.tb_runs = False
        self.state.tb_stimulus_created = False
        self.state.tb_error_message = ""
        # PyChecker Agent fields
        self.state.pychecker_extracted = False
        self.state.pychecker_runs = False
        self.state.pychecker_matches = False
        self.state.pychecker_error_message = ""
        self.current_turn = 0
        self.done = False


class PyCheckerEnvBatch:
    """Batch environment for PyChecker"""
    
    def __init__(
        self, 
        env_idx_list: List[int],
        env_indices: List[int], 
        rollout_idx_list: List[int], 
        samples: int, 
        max_turns: int, 
        config: dict, 
        mode="train", 
        *, 
        env_workers: List = None
    ):
        """
        Initialize batch environment
        
        Args:
            env_idx_list: List of environment indices
            env_indices: Indices of problems to load from dataset
            rollout_idx_list: List of rollout indices
            samples: Number of samples per problem
            max_turns: Maximum turns per episode
            config: Configuration dictionary
            mode: "train" or "validate"
            env_workers: Optional list of workers
        """
        # Load problems from dataset
        self.problem_list = self.load_pychecker_problems(env_indices, mode=mode, config=config)
        self.env_list = []
        
        if mode == "validate":
            rollout_idx_list = range(len(self.problem_list) * samples)
   
        if not self.problem_list:
            raise ValueError("Failed to load problems from dataset. Please check if the dataset is available.")

        # Create environments for each problem x samples
        for i, problem in enumerate(self.problem_list):
            state = PyCheckerEnvState(
                problem_input=problem["input"],
                spec=problem.get("spec", ""),
                golden_output=problem["output"],
                circuit_type=problem.get("circuit_type", "CMB"),
            )
            for s in range(samples):
                env = PyCheckerEnv(
                    env_idx=i, 
                    rollout_idx=rollout_idx_list[i*samples+s], 
                    max_turns=max_turns, 
                    config=None
                )
                env.state = copy.deepcopy(state)
                self.env_list.append(env)
                
        if len(self.env_list) != len(rollout_idx_list):
            raise ValueError(f"len(self.env_list)!=len(rollout_idx_list), {len(self.env_list)}!={len(rollout_idx_list)}")
    
    def load_pychecker_problems(
        self,
        env_indices: List[int],
        mode: str = "train",
        config: dict = None
    ) -> List[Dict]:
        """
        Load PyChecker problems from train.jsonl or test.jsonl

        Args:
            env_indices: Indices of problems to load (used to determine sample count for train mode)
            mode: "train" or "validate"
            config: Configuration dictionary

        Returns:
            List of problem dictionaries with 'input', 'output', 'circuit_type'
        """
        import os
        import random

        current_dir = os.path.dirname(os.path.abspath(__file__))

        # Determine which dataset file to use
        if mode == "train":
            dataset_path = os.path.join(current_dir, "dataset", "train.jsonl")
        else:
            dataset_path = os.path.join(current_dir, "dataset", "test.jsonl")

        logger.info(f"Loading PyChecker problems from {dataset_path} (mode={mode})")

        problems = []
        try:
            with open(dataset_path, 'r') as f:
                all_problems = [json.loads(line) for line in f if line.strip()]

            logger.info(f"Total problems available: {len(all_problems)}")

            # For train mode: randomly sample N problems where N = len(env_indices)
            # For validate mode: sample N problems sequentially where N = len(env_indices)
            if mode == "train":
                # Random sample based on the length of env_indices
                sample_num = len(env_indices)
                if sample_num > len(all_problems):
                    raise ValueError(f"Dataset has {len(all_problems)} samples, but {sample_num} requested")

                # Randomly sample without replacement
                indices = random.sample(range(len(all_problems)), sample_num)
                sampled_problems = [all_problems[idx] for idx in indices]
                logger.info(f"Randomly sampled {len(sampled_problems)} problems for training")
            else:
                
                sampled_problems = all_problems
                logger.info(f"Loaded {len(sampled_problems)} problems for validation")

            # Process problems
            for problem in sampled_problems:
                # Infer circuit type from output code:
                # SEQ circuits use @(posedge clk) or @(negedge clk)
                # CMB circuits use @(*) or assign statements
                output_code = problem["output"]
                has_posedge = "posedge" in output_code or "negedge" in output_code
                circuit_type = "SEQ" if has_posedge else "CMB"
                problems.append({
                    "input": problem["input"],
                    "spec": problem.get("spec", ""),
                    "output": problem["output"],
                    "circuit_type": circuit_type
                })

            logger.info(f"Successfully processed {len(problems)} problems")
            return problems

        except Exception as e:
            logger.error(f"Failed to load dataset from {dataset_path}: {e}")
            return []

