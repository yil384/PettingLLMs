defaults:
- ../ppo_trainer@models.model_0.ppo_trainer_config: eval
- ../ppo_trainer@models.model_1.ppo_trainer_config: eval
- ../ppo_trainer@models.model_2.ppo_trainer_config: eval
- _self_
specialization: full
lora_rank: 16
lora_alpha: 32
resource:
  nnodes: 1
  n_gpus_per_node: 4  # Default to 4, can be overridden by command line
  trust_remote_code: true
env:
  name: code_env
  dataset: code_contests
  benchmark: code_contests
  max_turns: 5
  resolve: false
  multi_modal: false
  batched_init: true
base_models:
  policy_0:
    path: Qwen/Qwen3-0.6B
    name: codeV_generator_model
  policy_1:
    path: Qwen/Qwen3-0.6B
    name: codeC_generator_model
  policy_2:
    path: Qwen/Qwen3-0.6B
    name: testbench_generator_model
agent_policy_configs:
  agent_configs:
    agent_0:
      name: codeV_generator
      policy_name: codeV_generator_model
      train_llm_config:
        enable_thinking: false
        temperature: 1.0
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: false
        temperature: 0.6
        top_p: 0.95
        top_k: 20
        min_p: 0.0
    agent_1:
      name: codeC_generator
      policy_name: codeC_generator_model
      train_llm_config:
        enable_thinking: false
        temperature: 1.0
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: false
        temperature: 0.6
        top_p: 0.95
        top_k: 20
        min_p: 0.0
    agent_2:
      name: testbench_generator
      policy_name: testbench_generator_model
      train_llm_config:
        enable_thinking: false
        temperature: 1.0
        top_p: 0.9
        top_k: 20
        min_p: 0.0
      val_llm_config:
        enable_thinking: false
        temperature: 0.6
        top_p: 0.95
        top_k: 20
        min_p: 0.0
multi_agent_interaction:
  turn_order:
  - codeV_generator
  - codeC_generator
  - testbench_generator
  parallel: true
training:
  device: cuda
  total_training_steps: 200
  project_name: pettingllms
  experiment_name: code_eval_multi_model
  logger:
  - console
  - wandb
  model_checkpoints_dir: checkpoints
  ray_wait_register_center_timeout: 300
  train_batch_size: 32
  train_sample_num: 8
  validate_sample_num: 1
  sample_temperature: 1
  val_freq: 10
  resample_freq: 3
  max_prompt_length: 4096
  max_response_length: 2048
  lora_rank: ${lora_rank}
  lora_alpha: ${lora_alpha}
models:
  model_0:
    path: ${base_models.policy_0.path}
    name: ${base_models.policy_0.name}
    ppo_trainer_config:
      filter_method: dapo
      filter_ratio: 0
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_0.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: 1  # Use 1 GPU per model since we have 3 models and only 4 GPUs
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}
  model_1:
    path: ${base_models.policy_1.path}
    name: ${base_models.policy_1.name}
    ppo_trainer_config:
      filter_method: dapo
      filter_ratio: 0
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_1.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: 1  # Use 1 GPU per model since we have 3 models and only 4 GPUs
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}
  model_2:
    path: ${base_models.policy_2.path}
    name: ${base_models.policy_2.name}
    ppo_trainer_config:
      filter_method: dapo
      filter_ratio: 0
      data:
        max_prompt_length: ${training.max_prompt_length}
        max_response_length: ${training.max_response_length}
      actor_rollout_ref:
        model:
          path: ${base_models.policy_2.path}
        rollout:
          temperature: ${training.sample_temperature}
          prompt_length: ${training.max_prompt_length}
          response_length: ${training.max_response_length}
          tensor_model_parallel_size: 1  # Use 1 GPU per model since we have 3 models and only 4 GPUs
        trainer:
          n_gpus_per_node: ${resource.n_gpus_per_node}
          n_training_gpus_per_node: ${resource.n_gpus_per_node}
